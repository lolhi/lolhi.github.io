---
title: "Lec09-딥러닝"
categories:
  - ML
last_modified_at: 2019-03-17T13:00:00+09:00
toc: true
---

## Lec09-1 XOR문제 딥러닝으로 풀기

  - 한개의 Logistic regression으로 XOR문제를 풀 수 없음
    - 수학적으로 증명됨

  - 여러개의 Logistic Regression으로 해결 가능
    - 하지만 Weight나 bias를 학습할 방법이 없음

  - Neural network에서 weight와 bias가 그림과 같은 값을 가질 때 XOR문제 해결 가능
  ![Lec09_1-1](/assets/image/Lec09_1-1.JPG)

  - 그림과 같은 Neural network를 구성했을때 어떻게 W1,W2,B1,b2를 학습시킬것인가?
  ![Lec09_1-2](/assets/image/Lec09_1-2.JPG)

## Lec09-x 미분 정리

  - 미분은 순간 변화율을 의미함.(기울기)

  - hypothesis에 x가 얼마나 영향을 미친다면...
    - ∂f/∂x = ∂f/∂g * ∂g/∂x로 기술 가능
    - Chain rule.
    ![Lec09_x-1](/assets/image/Lec09_x-1.JPG)

## Lec09-2 딥네트웍 학습시키기

  - W1,W2,B1,b2학습을 위해 Gradient Descent 알고리즘 사용
    - Gradient Descent 알고리즘을 사용하기 위해선 기울기가 필요한데 입력 값들이 출력값에 미친 영향을 찾아야함 -> Weight 조정가능 -> 계산량이 너무 많음

  - backpropagation 사용하여 가능
    - 출력값의 오류값을 뒤로...
    - forword 로 값을 입력한 다음 backword로 ...(chain rule 사용)
    ![Lec09_2-1](/assets/image/Lec09_2-1.JPG)
    - W와 b의 편미분값은 w,b값이 변할 때 몇배만큼 y값이 변하는지를 의미함(위 그림에서는 w값이 1 커지면 5배 커짐)
    - 아무리 복잡한 Neural network여도 chain rule을 사용해 단계적으로 구하면 모든 미분값을 구할 수 있음
    ![Lec09_2-2](/assets/image/Lec09_2-2.JPG)

## Lab09-1 Neural network for XOR

  ```python
  import tensorflow as tf
  import numpy as np

  x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)
  y_data = np.array([[0], [1], [1], [0]], dtype=np.float32)

  X = tf.placeholder(tf.float32)
  Y = tf.placeholder(tf.float32)

  # neural net을 구성하지 않고는 해결할 수 없음
  # W = tf.Variable(tf.random_normal([2, 1]), name='weight')
  # b = tf.Variable(tf.random_normal([1]), name='bias')
  W1 = tf.Variable(tf.random_normal([2, 2]), name='weight1')
  b1 = tf.Variable(tf.random_normal([2]), name='bias1')
  layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)

  W2 = tf.Variable(tf.random_normal([2, 1]), name='weight2')
  b2 = tf.Variable(tf.random_normal([1]), name='bias2')
  hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2)

  cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))
  train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost);

  predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)
  accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))

  with tf.Session() as sess:
      sess.run(tf.global_variables_initializer())

      for step in range(10001):
          sess.run(train, feed_dict={X: x_data, Y: y_data})
          if step % 100 == 0:
              print(step, sess.run(cost, feed_dict={X: x_data, Y: y_data}))

      h, c ,a = sess.run([hypothesis, predicted, accuracy], feed_dict={X: x_data, Y: y_data})
      print("\nHypothesis: ", h, "\nCorrect: ", c, "\nAccuracy: ", a)

  ```

  - 결과
  ```
  Hypothesis:  
  [[0.03289073]
  [0.97416365]
  [0.9741818 ]
  [0.03706551]]
  Correct:  
  [[0.]
  [1.]
  [1.]
  [0.]]
  Accuracy:  1.0
  ```
