---
title: "Lec10"
categories:
  - ML
last_modified_at: 2019-04-02T00:00:00+09:00
toc: true
---

## Lec10-1 Sigmoid보다 ReLU가 더 좋아

  - hidden layer 가 많으면 오히려 좋아지는게 아니라 정확도가 떨어짐
    - chain rule을 사용하여 계산하기 때문에 sigmoid를 통과한 값을 계속 곱해 결국 미분값이 0으로 수렴. -> 초기 단계의 출력값은 output에 영향을 거의 미치지 않음
    ![Lec10_1-1](/assets/image/Lec10_1-1.JPG)
    - 이를 Vanishing gradient라고 함.
    ![Lec10_1-2](/assets/image/Lec10_1-2.JPG)

  - Sigmoid처럼 linear 한 함수를 쓰지 않고 ReLU처럼 Non linear 함수를 쓰자
    - ReLU : 0 < x이면 0 나머지 그대로
    - 주의 : Logistic Regression을 위해서 마지막 레이어는 sigmoid 사용

## Lec10-2 Weight 초기화를 잘해보자

  - Vanishing Gradient 문제를 해결하기 위해서는 Weight값 초기화도 중요함  
  - RBM(Restricted Boatman Machine)
    - 잘 사용하지 않음
    1. 데이터와 W를 곱해 L1을 구함(1층만)
    2. 1번 계산에 사용된 W를 그대로 L1과 곱해 나온 X 추정값을 구함
    3. X 추정값과 실제 x값이 최대한 비슷할 정도가 될때까지 1,2번을 반복하여 W값을 조정함
  ![Lec10_2-1](/assets/image/Lec10_2-1.JPG)
  - RBM을 사용하여 한 층씩(L1과 L2의 W1, L2와 L3의 W2) W값을 초기화 (Deep Belif Network)
  - Xavier / He initialization
    - Xavier initialization : 입력값과 출력값 사이의 값을 입력값의 제곱근으로 나눠줌
    - He et al : 입력값과 출력값 사이의 값을 입력값 / 2 의 제곱근으로 나눠줌
    ![Lec10_2-2](/assets/image/Lec10_2-2.JPG)

  - 좋은 Weight를 찾는것은 아직 많이 연구중

## Lec10-3 Dropout과 앙상블

  - 학습데이터를 사용하여 정확도를 측정하면 99%가 되나 실험데이터를 사용하여 평가하면 낮은 정확도가 나옴 -> Overfitting
  - Overfitting을 해결하기 위해서는 많은 데이터를 사용하거나 Regularization을 사용해야함
  - Regularization
    ![Lec10_3-1](/assets/image/Lec10_3-1.JPG)

  - Dropout
    - 랜덤하게 뉴런들을 강제로 끔
    ![Lec10_3-2](/assets/image/Lec10_3-2.JPG)
    - 그 상태로 훈련을 진행하고, 마지막으로 모든 뉴런을 동원하여 확인
    - 학습할때만 Dropout 적용

  - 앙상블(Ensemble)
    - 기계가 많고 학습데이터가 많을때 사용 가능
    - k개의 모델을 만들어 각자 학습 시키고 마지막에 합함
    ![Lec10_3-3](/assets/image/Lec10_3-3.JPG)

## Lec10-4 레고처럼 network 모듈을 마음껏 쌍아보자

  - fast forward
    - 레이어의 출력을 바로 다음 레이어로 넘기는것이 아닌 두세단계 앞에 레이어에 보냄
    ![Lec10_4-1](/assets/image/Lec10_4-1.JPG)

  - Split & Merge
    - 레이어를 병렬로 배치해서 학습하고 다시 합치는 것을 반복하는 모델
    - CNN
    ![Lec10_4-2](/assets/image/Lec10_4-2.JPG)

  - Recurrent Network
    - 앞으로만 나가는것이 아닌 옆으로도....
    - RNN
    ![Lec10_4-3](/assets/image/Lec10_4-3.JPG)

## Lab10 NN, ReLU, Xavier, Dropout, and Adam

  ```python
  import tensorflow as tf
  import matplotlib.pyplot as plt
  import random

  from tensorflow.examples.tutorials.mnist import input_data

  tf.set_random_seed(777)  # reproducibility

  # Check out https://www.tensorflow.org/get_started/mnist/beginners for
  # more information about the mnist dataset
  mnist = input_data.read_data_sets("MNIST_data/", one_hot=True)

  # input place holders
  X = tf.placeholder(tf.float32, [None, 784])
  Y = tf.placeholder(tf.float32, [None, 10])

  # dropout (keep_prob) rate  0.7 on training, but should be 1 for testing
  keep_prob = tf.placeholder(tf.float32)

  # weights & bias for nn layers
  # W = tf.Variable(tf.random_normal([784, 10]))
  # b = tf.Variable(tf.random_normal([10]))

  # NN 사용(91% -> 95%)
  # weights & bias for nn layers

  # Xavier(in ~ out / √in) 사용(95% -> 98%)
  #W1 = tf.Variable(tf.random_normal([784, 256]))
  #W1 = tf.get_variable("W1", shape=[784,256],
  #                    initializer=tf.contrib.layers.xavier_initializer())
  #b1 = tf.Variable(tf.random_normal([256]))
  #L1 = tf.nn.relu(tf.matmul(X, W1) + b1)

  #W2 = tf.Variable(tf.random_normal([256, 256]))
  #W2 = tf.get_variable("W2", shape=[256,256],
  #                    initializer=tf.contrib.layers.xavier_initializer())
  #b2 = tf.Variable(tf.random_normal([256]))
  #L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)

  #W3 = tf.Variable(tf.random_normal([256, 10]))
  #W3 = tf.get_variable("W3", shape=[256,10],
  #                    initializer=tf.contrib.layers.xavier_initializer())
  #b3 = tf.Variable(tf.random_normal([10]))

  # depp & dropout 사용(98.15% ->  98.4%)
  # weights & bias for nn layers
  # http://stackoverflow.com/questions/33640581/how-to-do-xavier-initialization-on-tensorflow
  W1 = tf.get_variable("W1", shape=[784, 512],
                       initializer=tf.contrib.layers.xavier_initializer())
  b1 = tf.Variable(tf.random_normal([512]))
  L1 = tf.nn.relu(tf.matmul(X, W1) + b1)
  L1 = tf.nn.dropout(L1, keep_prob=keep_prob)

  W2 = tf.get_variable("W2", shape=[512, 512],
                       initializer=tf.contrib.layers.xavier_initializer())
  b2 = tf.Variable(tf.random_normal([512]))
  L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)
  L2 = tf.nn.dropout(L2, keep_prob=keep_prob)

  W3 = tf.get_variable("W3", shape=[512, 512],
                       initializer=tf.contrib.layers.xavier_initializer())
  b3 = tf.Variable(tf.random_normal([512]))
  L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)
  L3 = tf.nn.dropout(L3, keep_prob=keep_prob)

  W4 = tf.get_variable("W4", shape=[512, 512],
                       initializer=tf.contrib.layers.xavier_initializer())
  b4 = tf.Variable(tf.random_normal([512]))
  L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)
  L4 = tf.nn.dropout(L4, keep_prob=keep_prob)

  W5 = tf.get_variable("W5", shape=[512, 10],
                       initializer=tf.contrib.layers.xavier_initializer())
  b5 = tf.Variable(tf.random_normal([10]))


  # parameters
  learning_rate = 0.001
  batch_size = 100
  num_epochs = 50
  num_iterations = int(mnist.train.num_examples / batch_size)

  hypothesis = tf.matmul(L4, W5) + b5

  # define cost/loss & optimizer
  cost = tf.reduce_mean(
      tf.nn.softmax_cross_entropy_with_logits_v2(
          logits=hypothesis, labels=tf.stop_gradient(Y)
      )
  )
  # GradientDescentOptimizer 말고 여러 optimizer가 있음
  train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)

  correct_prediction = tf.equal(tf.argmax(hypothesis, axis=1), tf.argmax(Y, axis=1))
  accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

  # train my model
  with tf.Session() as sess:
      # initialize
      sess.run(tf.global_variables_initializer())

      for epoch in range(num_epochs):
          avg_cost = 0

          for iteration in range(num_iterations):
              batch_xs, batch_ys = mnist.train.next_batch(batch_size)
              _, cost_val = sess.run([train, cost], feed_dict={X: batch_xs, Y: batch_ys, keep_prob: 0.7})
              avg_cost += cost_val / num_iterations

          print(f"Epoch: {(epoch + 1):04d}, Cost: {avg_cost:.9f}")

      print("Learning Finished!")

      # Test model and check accuracy
      print(
          "Accuracy:",
          sess.run(accuracy, feed_dict={X: mnist.test.images, Y: mnist.test.labels, keep_prob: 1}),
      )

      # Get one and predict
      r = random.randint(0, mnist.test.num_examples - 1)

      print("Label: ", sess.run(tf.argmax(mnist.test.labels[r : r + 1], axis=1)))
      print(
          "Prediction: ",
          sess.run(
              tf.argmax(hypothesis, axis=1), feed_dict={X: mnist.test.images[r : r + 1], keep_prob: 1}
          ),
      )

  #    plt.imshow(
  #        mnist.test.images[r : r + 1].reshape(28, 28),
  #        cmap="Greys",
  #        interpolation="nearest",
  #    )
  #    plt.show()

  ```
