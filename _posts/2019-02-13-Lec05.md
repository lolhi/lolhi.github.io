---
title: "Lec05-Logistic Classfication"
categories:
  - ML
last_modified_at: 2019-02-12T13:00:00+09:00
toc: true
---

## Lec 05-1 - Logistic Classfication의 가설함수 정의

  - 이전까지 했던 Regression은 값을 추측하는 것
  - (Binary) Classfication은 정해진 category를 고르는 것
  - 기계적으로 학습하기 위해 0, 1로 encoding 함

  - Linear Regression으로 Classfication 문제를 해결 할 수가 없음
    - Linear Regression을 미리 학습시켜두고 나서 어마어마하게 큰 수가 데이터로 들어오면 기울기가 줄어 정상적인 Classfication 불가
    ![Lec05-1](/assets/image/Lec05-1.JPG)
    - Classfication을 위해 hypothesis의 값이 1이나 0이 나와야 하는데 Linear Regression의 hypothesis(H(x) = Wx + b)는 1보다 크거나 0보다 작은 수가 나올 수 있음

  - 이 문제를 해결하기 위해선 H(x)의 값이 무엇이든 0 ~ 1의 값으로 변환해주어야 함
    - Sigmoid(Logistic) function을 사용
    ![Lec05-2](/assets/image/Lec05-2.JPG)
    ![Lec05-3](/assets/image/Lec05-3.JPG)

## Lec 05-2 - Logistic Classfication의 cost함수 설명

  - Logistic funtion의 cost funtion
    - Linear Regression의 cost funtion을 그대로 사용하면 cost funtion의 그래프가 이상하게 그려짐
    ![Lec05-4](/assets/image/Lec05-4.JPG)
      - 중간중간 기울기가 없어지는 부분이 있기 때문에 경사하강법 알고리즘 사용 불가
    - 따라서 다음과 같은 cost funtion을 사용함
    ![Lec05-5](/assets/image/Lec05-5.JPG)
      - y=1 일때 H(x)도 1이면 cost는 0, H(x)가 0이면 cost는 무한대
      - y=0 일때 H(x)도 0이면 cost는 0, H(x)가 1이면 cost는 무한대
      - 프로그래밍에 사용하기 위해 다음과 같이 식을 변경
      ![Lec05-6](/assets/image/Lec05-6.JPG)
    - Gradient Descent algorithm을 사용하기 위해 cost를 미분해야하는데 컴퓨터가 알아서 해줌(복잡함)
    ![Lec05-7](/assets/image/Lec05-7.JPG)

## Lab 05-1 - Tensorflow로 Logistic Classfication 구현하기

  ```python
  import tensorflow as tf

  x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]
  y_data = [[0], [0], [0], [1], [1], [1]]

  X = tf.placeholder(tf.float32, shape=[None, 2])
  Y = tf.placeholder(tf.float32, shape=[None, 1])
  W = tf.Variable(tf.random_normal([2, 1]), name='weight')
  b = tf.Variable(tf.random_normal([1]), name='bias')

  # H(x)= 1/1+e^(-x)
  # tf.div(1., 1. + tf.exp(tf.matmul(X, W) + b))
  hypothesis = tf.sigmoid(tf.matmul(X, W) + b)

  # cost = -1/m∑(ylog(H(x)) + (1 - y)log(1 - H(x)))
  cost = -tf.reduce_mean(Y *  tf.log(hypothesis) +
                      (1 - Y) * tf.log(1 - hypothesis))

  # minimize
  train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)

  # Accuracy computation
  # True if hypothesis>0.5 else False
  # True면 1.0, False면 0.0으로 casting
  predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)
  accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))

  with tf.Session() as sess:
      sess.run(tf.global_variables_initializer())

      for step in range(10001):
          cost_val, _ = sess.run([cost, train], feed_dict={X: x_data, Y: y_data})
          if step % 200 == 0:
              print(step, cost_val)

      #Accuracy Report
      h, c, a = sess.run([hypothesis, predicted, accuracy],
                          feed_dict={X: x_data, Y:y_data})
      print("\nHypothesis: ", h, "\n Correct (Y): ", c, "\nAccuracy: ", a)

  ```

## Lab 05-2 - Tensorflow로 Logistic Classfication 구현하기(데이터 사용)

  - 데이터는 https://github.com/hunkim/DeepLearningZeroToAll/ 에서 다운 가능

  ```python
  import tensorflow as tf
  import numpy as np

  # loadtext : text파일을 읽음. delimiter를 사용해 구분가능
  xy = np.loadtxt('C:\\Users\\YJ\\Desktop\\workspace\\PythonProject\\ML_Lec\\Lec05 - Tensorflow로 Logistic Classification 구현하기\\data-03-diabetes.csv', delimiter=',', dtype=np.float32)

  # [:, 0:-1] : row(행)은 전부 가져오고 column(열)은 마지막 한개(-1)를 제외하고 다 가져온다.
  x_data = xy[:, 0:-1]
  # [:,[-1]] : row(행)은 전부 가져오고 column(열)은 마지막 한개(-1)만 가져온다
  y_data = xy[:,[-1]]

  X = tf.placeholder(tf.float32, shape=[None, 8])
  Y = tf.placeholder(tf.float32, shape=[None, 1])
  W = tf.Variable(tf.random_normal([8, 1]), name='weight')
  b = tf.Variable(tf.random_normal([1]), name='bias')

  # H(x)= 1/1+e^(-x)
  # tf.div(1., 1. + tf.exp(tf.matmul(X, W) + b))
  hypothesis = tf.sigmoid(tf.matmul(X, W) + b)

  # cost = -1/m∑(ylog(H(x)) + (1 - y)log(1 - H(x)))
  cost = -tf.reduce_mean(Y *  tf.log(hypothesis) +
                      (1 - Y) * tf.log(1 - hypothesis))

  # minimize
  train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)

  # Accuracy computation
  # True if hypothesis>0.5 else False
  # True면 1.0, False면 0.0으로 casting
  predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)
  accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))

  with tf.Session() as sess:
      sess.run(tf.global_variables_initializer())

      for step in range(10001):
          cost_val, _ = sess.run([cost, train], feed_dict={X: x_data, Y: y_data})
          if step % 200 == 0:
              print(step, cost_val)

      #Accuracy Report
      h, c, a = sess.run([hypothesis, predicted, accuracy],
                          feed_dict={X: x_data, Y:y_data})
      print("\nHypothesis: ", h, "\n Correct (Y): ", c, "\nAccuracy: ", a)

  ```
