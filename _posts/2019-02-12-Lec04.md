---
title: "Lec04-Multi-Variable Linear Regression"
categories:
  - ML
last_modified_at: 2019-02-12T13:00:00+09:00
toc: true
---

## Lec 04 - Multi-Variable Linear Regression

  - 한개의 데이터가 아닌 여러개의 데이터일때의 regression
  ![Lec04-1](/assets/image/Lec04-1.JPG)
  - Hypothesis 및 cost funtion
  ![Lec04-2](/assets/image/Lec04-2.JPG)
  - 데이터가 너무 많아지면 표현하기 힘듬
    -> Matrix로 표현하여 Matrix의 곱 사용
    ![Lec04-3](/assets/image/Lec04-3.JPG)
  - Matrix 사용 시 instance갯수가 많아도 한번에 계산 가능

## Lab 04-1 - Multi-Variable Linear Regression을 Tensorflow에서 구현하기

  ```python
  import tensorflow as tf

  x_data = [[73., 80., 75.], [93., 88., 93.],
              [89., 91., 90.], [96., 98., 100.,], [73., 66., 70.]]
  y_data = [[152.], [185.], [180.], [196.], [142.]]

  # None은 N개를 의미
  X = tf.placeholder(tf.float32, shape=[None,3])
  Y = tf.placeholder(tf.float32, shape=[None,1])

  W = tf.Variable(tf.random_normal([3, 1]),name='weight')
  b = tf.Variable(tf.random_normal([1]),name='bias')

  hypothesis = tf.matmul(X,W) + b

  # cost funtion
  cost = tf.reduce_mean(tf.square(hypothesis - Y))
  # Minimize
  optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)
  train = optimizer.minimize(cost)

  sess = tf.Session()
  sess.run(tf.global_variables_initializer())

  for step in range(2001):
      cost_val, hypothesis_val, _ = sess.run(
          [cost, hypothesis, train], feed_dict={X: x_data, Y: y_data})
      if step % 10 == 0:
          print(step, "Cost : ", cost_val, "\n hypothesis :\n", hypothesis_val)

  ```

## Lab 04-2 - Tensorflow로 파일에서 데이타 읽어오기

  - 데이터는 https://github.com/hunkim/DeepLearningZeroToAll/ 에서 다운 가능

  ```python
  import tensorflow as tf
  import numpy as np

  # loadtext : text파일을 읽음. delimiter를 사용해 구분가능
  xy = np.loadtxt('C:\\Users\\YJ\\Desktop\\workspace\\PythonProject\\ML_Lec\\Lec04 - Multi-Variable Linear Regression을 Tensorflow에서 구현\\data-01-test-score.csv', delimiter=',', dtype=np.float32)

  # [:, 0:-1] : row(행)은 전부 가져오고 column(열)은 마지막 한개(-1)를 제외하고 다 가져온다.
  x_data = xy[:, 0:-1]
  # [:,[-1]] : row(행)은 전부 가져오고 column(열)은 마지막 한개(-1)만 가져온다
  y_data = xy[:,[-1]]

  print(x_data.shape, x_data, len(x_data))
  print(y_data.shape, y_data)

  X = tf.placeholder(tf.float32, shape=[None,3])
  Y = tf.placeholder(tf.float32, shape=[None,1])

  W = tf.Variable(tf.random_normal([3,1]), name='weight')
  b = tf.Variable(tf.random_normal([1]), name='bias')

  hypothesis = tf.matmul(X,W) + b

  cost = tf.reduce_mean(tf.square(hypothesis - Y))

  optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-5)
  train = optimizer.minimize(cost)

  sess = tf.Session()
  sess.run(tf.global_variables_initializer())

  for step in range(2001):
      cost_val, hy_val, _ = sess.run(
          [cost, hypothesis, train],
          feed_dict={X: x_data, Y: y_data})

      if step % 10 == 0:
          print(step, "Cost : ", cost_val, "\n hypothesis :\n", hy_val)

  #Ask my score
  print("Your score will be ", sess.run(hypothesis, feed_dict={X: [[100, 70, 101]]}))

  ```
