---
title: "Lec05-Logistic Classfication"
categories:
  - ML
last_modified_at: 2019-02-14T13:00:00+09:00
toc: true
---
## Lec 06 - Softmax Classfication: Multinomial Classfication

  - Logistic Classfication의 개념을 Multinomial Classfication에 적용
  ![Lec06-1](/assets/image/Lec06-1.JPG)
  - 세개의 독립된 Classfication을 해도 되지만 하나의 벡터 묶어서 하면 바로 계산이 가능하고 독립된것들처럼 동작
  ![Lec06-2](/assets/image/Lec06-2.JPG)
  - 벡터를 계산해서 나온 값들은 0 ~ 1사이의 값들이 아님
    -> Softmax를 사용하여 0 ~ 1로 변환(확률)
    ![Lec06-3](/assets/image/Lec06-3.JPG)
    ![Lec06-4](/assets/image/Lec06-4.JPG)
  - Softmax를 사용하여 구한 확률을 one-hot encoding으로 변환
  ![Lec06-5](/assets/image/Lec06-5.JPG)

  - Softmax Classfication의 cost funtion
  ![Lec06-6](/assets/image/Lec06-6.JPG)
  - Classfication의 예측결과가 틀렸으면 cost는 무한대, 맞았으면 0
  ![Lec06-7](/assets/image/Lec06-7.JPG)

## Lab 06-1 - Tensorflow로 Softmax Classfication 구현하기

  ```python
  import tensorflow as tf

  x_data = [[1, 2, 1, 1],
            [2, 1, 3, 2],
            [3, 1, 3, 4],
            [4, 1, 5, 5],
            [1, 7, 5, 5],
            [1, 2, 5, 6],
            [1, 6, 6, 6],
            [1, 7, 7, 7]]

  y_data = [[0, 0, 1],
            [0, 0, 1],
            [0, 0, 1],
            [0, 1, 0],
            [0, 1, 0],
            [0, 1, 0],
            [1, 0, 0],
            [1, 0, 0]]

  X = tf.placeholder(tf.float32, shape=[None, 4])
  Y = tf.placeholder(tf.float32, shape=[None, 3])
  nb_classes = 3

  W = tf.Variable(tf.random_normal([4, nb_classes]), name='weigth')
  b = tf.Variable(tf.random_normal([nb_classes]), name='bias')

  # logit
  hypothesis = tf.nn.softmax(tf.matmul(X,W) + b)

  # cross entropy cost funtion
  # cost_i = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y)
  # cost = tf.reduce_mean(cost_i)
  cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))

  optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)

  with tf.Session() as sess:
      sess.run(tf.global_variables_initializer())

      for step in range(2001):
          sess.run(optimizer, feed_dict={X: x_data, Y: y_data})
          if step % 200 == 0:
              print (step, sess.run(cost, feed_dict={X: x_data, Y: y_data}))

      print('--------------')
      # Testing & One-hot encoding
      a = sess.run(hypothesis, feed_dict={X: [[1, 11, 7, 9]]})
      print(a, sess.run(tf.argmax(a, 1)))
      print('--------------')
      b = sess.run(hypothesis, feed_dict={X: [[1, 3, 4, 3]]})
      print(b, sess.run(tf.argmax(b, 1)))
      print('--------------')
      c = sess.run(hypothesis, feed_dict={X: [[1, 1, 0, 1]]})
      print(c, sess.run(tf.argmax(c, 1)))
      print('--------------')
      all = sess.run(hypothesis, feed_dict={X: [[1, 11, 7, 9], [1, 3, 4, 3], [1, 1, 0, 1]]})
      print(all, sess.run(tf.argmax(all, 1)))

  ```

## Lab 06-2 - Tensorflow로 Softmax Classfication 구현하기(데이터사용)

  - 데이터는 https://github.com/hunkim/DeepLearningZeroToAll/ 에서 다운 가능

  ```python
  import tensorflow as tf
  import numpy as np

  # Predicting animal type based on various features
  xy = np.loadtxt('C:\\Users\\YJ\\Desktop\\workspace\\PythonProject\\ML_Lec\\Lec06- Tensorflow로 softmax Classification 구현하기\\data-04-zoo.csv', delimiter=',', dtype=np.float32)
  x_data = xy[:, 0:-1]
  y_data = xy[:, [-1]]

  print(x_data.shape, y_data.shape)

  nb_classes = 7  # 0 ~ 6

  X = tf.placeholder(tf.float32, [None, 16])
  # tf.one_hot()을 사용하기 위해 int32 사용
  Y = tf.placeholder(tf.int32, [None, 1])  # 0 ~ 6

  Y_one_hot = tf.one_hot(Y, nb_classes)  # one hot
  print("one_hot:", Y_one_hot)
  # reshape시 -1은 모두를 의미
  Y_one_hot = tf.reshape(Y_one_hot, [-1, nb_classes])
  print("reshape one_hot:", Y_one_hot)

  W = tf.Variable(tf.random_normal([16, nb_classes]), name='weight')
  b = tf.Variable(tf.random_normal([nb_classes]), name='bias')

  # tf.nn.softmax computes softmax activations
  # softmax = exp(logits) / reduce_sum(exp(logits), dim)
  logits = tf.matmul(X, W) + b
  hypothesis = tf.nn.softmax(logits)

  # Cross entropy cost/loss
  cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,
                                                                   labels=tf.stop_gradient([Y_one_hot])))
  optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)

  prediction = tf.argmax(hypothesis, 1)
  correct_prediction = tf.equal(prediction, tf.argmax(Y_one_hot, 1))
  accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

  # Launch graph
  with tf.Session() as sess:
      sess.run(tf.global_variables_initializer())

      for step in range(2001):
          _, cost_val, acc_val = sess.run([optimizer, cost, accuracy], feed_dict={X: x_data, Y: y_data})

          if step % 100 == 0:
              print("Step: {:5}\tCost: {:.3f}\tAcc: {:.2%}".format(step, cost_val, acc_val))

      # Let's see if we can predict
      pred = sess.run(prediction, feed_dict={X: x_data})
      # y_data: (N,1) = flatten => (N, ) matches pred.shape
      for p, y in zip(pred, y_data.flatten()):
          print("[{}] Prediction: {} True Y: {}".format(p == int(y), p, int(y)))

  ```
