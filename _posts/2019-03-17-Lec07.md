---
title: "Lec07-학습 rate, Overfitting, 일반화(Regularization) 그리고 Training/ Testing data Set"
categories:
  - ML
last_modified_at: 2019-03-17T13:00:00+09:00
toc: true
---

## Lec 07-1 학습 rate, Overfitting, 일반화(Regularization)

  - Learning rate
    - Large Learning rate: Overshooting(Learning rate가 너무 커 cost가 줄어들지 않고 바깥으로 튕겨나감) 유발
    - Small Learning rate: 너무 오래걸려 gradient가 최저값이 아님에도 step이 끝남
    - 이런현상을 방지하기 위해서 cost값을 찍어봐야함

  - Data preprocessing for gradient descent
    - 학습에 사용되는 데이터가 서로 편차가 심히면 튀어나가는 상황이 발생할 수 있음
    - 그래서 데이터를 정규화 해줘야함(Zero-centered data or normalized data)
    - Standardization 방법
    ![Lec07-1](/assets/image/Lec07-1.JPG)

  - Overfitting
    - 학습데이터에 너무 맞는 모델을 만들어 다른 데이터가 들어왔을 때 정상적으로 처리하지 못함
    - 해결 방법은 많은 데이터, 중복된 features 제거 , Regularization
    - Regularization: 너무 큰 Weight 값을 가지게 하지 않는 것
    ![Lec07-2](/assets/image/Lec07-2.JPG)
    - cost funtion에 특정 값을 더해 cost 값 최적화

## Lec 07-2 Traning/ Testing data set

  - data set을 전부 training하는데 사용하면 문제가 발생할 수 있음
    - 시험을 치는데 지난 시험 문제와 똑같은 문제로 시험치는것과 똑같음
  - 모델의 정확성을 높히기 위해서 data set의 70%는 training set으로, 30%는 test set으로 분리
  - test set은 숨겨져 있고 training set으로 학습
  ![Lec07_1-1](/assets/image/Lec07_1-1.JPG)


## Lab 07 - Meet MNIST dataset

  ```python
  import tensorflow as tf
  import matplotlib.pyplot as plt
  import random

  from tensorflow.examples.tutorials.mnist import input_data
  nb_classes = 10
  mnist = input_data.read_data_sets("MNIST_data/", one_hot=True)

  # MNIST는 28*28(784)의 shape을 가짐
  X = tf.placeholder(tf.float32, [None,784])
  Y = tf.placeholder(tf.float32, [None, nb_classes])

  W = tf.Variable(tf.random_normal([784, nb_classes]))
  b = tf.Variable(tf.random_normal([nb_classes]))

  hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)

  cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))
  optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)

  # Test model
  is_correct = tf.equal(tf.arg_max(hypothesis, 1), tf.arg_max(Y, 1))
  # Calculate accuracy
  accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))

  # parameters
  training_epochs = 15
  batch_size = 100
  total_batch = int(mnist.train.num_examples / batch_size)

  with tf.Session() as sess:
      sess.run(tf.global_variables_initializer())
      for epoch in range(training_epochs):
          avg_cost = 0

          for i in range(total_batch):
              batch_xs, batch_ys = mnist.train.next_batch(batch_size)
              c, _ = sess.run([cost,optimizer], feed_dict={X: batch_xs, Y: batch_ys})
              avg_cost += c / total_batch

      print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))

      print("Learning finished")

      # Test the model using test sets
      print(
          "Accuracy: ",
          accuracy.eval(
              session=sess, feed_dict={X: mnist.test.images, Y: mnist.test.labels}
          ),
      )

      # Get one and predict
      r = random.randint(0, mnist.test.num_examples - 1)
      print("Label: ", sess.run(tf.argmax(mnist.test.labels[r: r + 1], 1)))
      print(
          "Prediction: ",
          sess.run(tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r: r + 1]}),
      )

      plt.imshow(
          mnist.test.images[r: r + 1].reshape(28, 28),
          cmap="Greys",
          interpolation="nearest",
      )
      plt.show()

  ```
