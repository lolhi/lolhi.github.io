---
title: "Lec12 Recurrent Neural Network"
categories:
  - ML
last_modified_at: 2019-04-03T00:00:00+09:00
toc: true
---

## Lec12 NN의 꽃 RNN 이야기

  - Sequence data
    - 음성인식, 자연어등 하나의 단어로 이루어져 있는게 아니라 Sequence로 이루어져 있음
    - 전에 있던 단어가 다음 단어에 영향을 줌
    ![Lec12-1](/assets/image/Lec12-1.JPG)
    - New state(ht)를 구하기 위해서는 이전 상태의 state와 t타임의 입력을 이용해 계산한다.
    ![Lec12-2](/assets/image/Lec12-2.JPG)
    - 모든 RNN에 사용되는 F functuon은 모두 같은것을 사용
    - (Vanilla) Recurrent Neural Network는 activate func으로 tanh를 사용하고 각 입력값에 W를 곱해준다. t타임의 y를 구하기위해서도 t타임의 h에 W를 곱해준다
    ![Lec12-3](/assets/image/Lec12-3.JPG)

  - RNN으로 학습하고 싶은것은 한 문자(h)를 입력했을때 다음문자가 무엇이 올지 학습(Language model)
  ![Lec12-4](/assets/image/Lec12-4.JPG)

## Lab12-1 Hi Hello Training

```python
import tensorflow as tf
import numpy as np
tf.set_random_seed(777)  # reproducibility

idx2char = ['h', 'i', 'e', 'l', 'o']
# Teach hello: hihell -> ihello
x_data = [[0, 1, 0, 2, 3, 3]]   # hihell
x_one_hot = [[[1, 0, 0, 0, 0],   # h 0
              [0, 1, 0, 0, 0],   # i 1
              [1, 0, 0, 0, 0],   # h 0
              [0, 0, 1, 0, 0],   # e 2
              [0, 0, 0, 1, 0],   # l 3
              [0, 0, 0, 1, 0]]]  # l 3

y_data = [[1, 0, 2, 3, 3, 4]]    # ihello

num_classes = 5
input_dim = 5  # one-hot size
hidden_size = 5  # output from the LSTM. 5 to directly predict one-hot
batch_size = 1   # one sentence
sequence_length = 6  # |ihello| == 6
learning_rate = 0.1

X = tf.placeholder(
    tf.float32, [None, sequence_length, input_dim])  # X one-hot
Y = tf.placeholder(tf.int32, [None, sequence_length])  # Y label

cell = tf.contrib.rnn.BasicLSTMCell(num_units=hidden_size, state_is_tuple=True)
initial_state = cell.zero_state(batch_size, tf.float32)
outputs, _states = tf.nn.dynamic_rnn(
    cell, X, initial_state=initial_state, dtype=tf.float32)

# FC layer
X_for_fc = tf.reshape(outputs, [-1, hidden_size])
# fc_w = tf.get_variable("fc_w", [hidden_size, num_classes])
# fc_b = tf.get_variable("fc_b", [num_classes])
# outputs = tf.matmul(X_for_fc, fc_w) + fc_b
outputs = tf.contrib.layers.fully_connected(
    inputs=X_for_fc, num_outputs=num_classes, activation_fn=None)

# reshape out for sequence_loss
outputs = tf.reshape(outputs, [batch_size, sequence_length, num_classes])

weights = tf.ones([batch_size, sequence_length])
sequence_loss = tf.contrib.seq2seq.sequence_loss(
    logits=outputs, targets=Y, weights=weights)
loss = tf.reduce_mean(sequence_loss)
train = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)

prediction = tf.argmax(outputs, axis=2)

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    for i in range(50):
        l, _ = sess.run([loss, train], feed_dict={X: x_one_hot, Y: y_data})
        result = sess.run(prediction, feed_dict={X: x_one_hot})
        print(i, "loss:", l, "prediction: ", result, "true Y: ", y_data)

        # print char using dic
        result_str = [idx2char[c] for c in np.squeeze(result)]
        print("\tPrediction str: ", ''.join(result_str))

```

## Lab12-3 Long Sequence RNN

```python
from __future__ import print_function

import tensorflow as tf
import numpy as np
from tensorflow.contrib import rnn

tf.set_random_seed(777)  # reproducibility

sentence = ("if you want to build a ship, don't drum up people together to "
            "collect wood and don't assign them tasks and work, but rather "
            "teach them to long for the endless immensity of the sea.")

char_set = list(set(sentence))
char_dic = {w: i for i, w in enumerate(char_set)}

data_dim = len(char_set)
hidden_size = len(char_set)
num_classes = len(char_set)
sequence_length = 10  # Any arbitrary number
learning_rate = 0.1

dataX = []
dataY = []
for i in range(0, len(sentence) - sequence_length):
    x_str = sentence[i:i + sequence_length]
    y_str = sentence[i + 1: i + sequence_length + 1]
    print(i, x_str, '->', y_str)

    x = [char_dic[c] for c in x_str]  # x str to index
    y = [char_dic[c] for c in y_str]  # y str to index

    dataX.append(x)
    dataY.append(y)

batch_size = len(dataX)

X = tf.placeholder(tf.int32, [None, sequence_length])
Y = tf.placeholder(tf.int32, [None, sequence_length])

# One-hot encoding
X_one_hot = tf.one_hot(X, num_classes)
print(X_one_hot)  # check out the shape

cell = rnn.BasicLSTMCell(hidden_size, state_is_tuple=True)

# outputs: unfolding size x hidden size, state = hidden size
outputs, _states = tf.nn.dynamic_rnn(cell, X_one_hot, dtype=tf.float32)

# All weights are 1 (equal weights)
weights = tf.ones([batch_size, sequence_length])

sequence_loss = tf.contrib.seq2seq.sequence_loss(
    logits=outputs, targets=Y, weights=weights)
mean_loss = tf.reduce_mean(sequence_loss)
train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(mean_loss)

sess = tf.Session()
sess.run(tf.global_variables_initializer())

for i in range(500):
    _, l, results = sess.run(
        [train_op, mean_loss, outputs], feed_dict={X: dataX, Y: dataY})
    for j, result in enumerate(results):
        index = np.argmax(result, axis=1)
        print(i, j, ''.join([char_set[t] for t in index]), l)

# Let's print the last char of each result to check it works
results = sess.run(outputs, feed_dict={X: dataX})
for j, result in enumerate(results):
    index = np.argmax(result, axis=1)
    if j is 0:  # print all for the first result to make a sentence
        print(''.join([char_set[t] for t in index]), end='')
    else:
        print(char_set[index[-1]], end='')
```

```
499 168 tn to  te  1.9057525
499 169 n to  te   1.9057525
f you want to eu ld a ss p  dondt aru  up pe ple to e h r to eollect  ood and dondt ass gn th   to ss and dor   ou  rath r te ch th   to lend tor to  end e s im  nsst     th  te  
[Finished in 27.558s]
```
  - 정상적으로 작동하지 않음

## Lab12-4 RNN with long sequences: Stacked RNN + Softmax layer

  - RNN Sequnce의 완성
  - 3번 Lab의 문제점
    1. 복잡하고 많은 데이터를 다루기에는 RNN cell 이 너무 작음
      - Stack RNN을 사용해 여러 층의 cell을 만든다.
      ![Lec12_4-1](/assets/image/Lec12_4-1.JPG)
    2. RNN의 출력을 그대로 사용하니 성능이 좋지 않음
      - RNN의 출력을 그대로 사용하지 않고 FC를 사용해 Softmax 처리를 하자!
      ![Lec12_4-2](/assets/image/Lec12_4-2.JPG)

```python
from __future__ import print_function

import tensorflow as tf
import numpy as np
from tensorflow.contrib import rnn

tf.set_random_seed(777)  # reproducibility

sentence = ("if you want to build a ship, don't drum up people together to "
            "collect wood and don't assign them tasks and work, but rather "
            "teach them to long for the endless immensity of the sea.")

char_set = list(set(sentence))
char_dic = {w: i for i, w in enumerate(char_set)}

data_dim = len(char_set)
hidden_size = len(char_set)
num_classes = len(char_set)
sequence_length = 10  # Any arbitrary number
learning_rate = 0.1

dataX = []
dataY = []
for i in range(0, len(sentence) - sequence_length):
    x_str = sentence[i:i + sequence_length]
    y_str = sentence[i + 1: i + sequence_length + 1]
    print(i, x_str, '->', y_str)

    x = [char_dic[c] for c in x_str]  # x str to index
    y = [char_dic[c] for c in y_str]  # y str to index

    dataX.append(x)
    dataY.append(y)

batch_size = len(dataX)

X = tf.placeholder(tf.int32, [None, sequence_length])
Y = tf.placeholder(tf.int32, [None, sequence_length])

# One-hot encoding
X_one_hot = tf.one_hot(X, num_classes)
print(X_one_hot)  # check out the shape

cell = rnn.BasicLSTMCell(hidden_size, state_is_tuple=True)
cell = rnn.MultiRNNCell([cell]*2, state_is_tuple=True)

# outputs: unfolding size x hidden size, state = hidden size
outputs, _states = tf.nn.dynamic_rnn(cell, X_one_hot, dtype=tf.float32)

# FC
X_for_sotfmax = tf.reshape(outputs, [-1, hidden_size])

#softmax_w = tf.get_variable("softmax_w", [hidden_size, num_classes])
#softmax_b = tf.get_variable("softmax_b", [num_classes])

#outputs = tf.matmul(X_for_sotfmax, softmax_w) + softmax_b;

# 위에 주석처리한 코드하고 아래 코드하고 성능차이가 있음..

outputs = tf.contrib.layers.fully_connected(X_for_sotfmax, num_classes, activation_fn=None)
outputs = tf.reshape(outputs, [batch_size, sequence_length, num_classes])

# All weights are 1 (equal weights)
weights = tf.ones([batch_size, sequence_length])

sequence_loss = tf.contrib.seq2seq.sequence_loss(
    logits=outputs, targets=Y, weights=weights)
mean_loss = tf.reduce_mean(sequence_loss)
train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(mean_loss)

sess = tf.Session()
sess.run(tf.global_variables_initializer())

for i in range(500):
    _, l, results = sess.run(
        [train_op, mean_loss, outputs], feed_dict={X: dataX, Y: dataY})
    for j, result in enumerate(results):
        index = np.argmax(result, axis=1)
        print(i, j, ''.join([char_set[t] for t in index]), l)

# Let's print the last char of each result to check it works
results = sess.run(outputs, feed_dict={X: dataX})
for j, result in enumerate(results):
    index = np.argmax(result, axis=1)
    if j is 0:  # print all for the first result to make a sentence
        print(''.join([char_set[t] for t in index]), end='')
    else:
        print(char_set[index[-1]], end='')

```

```
499 167  of the se 0.2295524
499 168 tf the sea 0.2295524
499 169   the sea. 0.2295524
m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.
[Finished in 33.295s]

```

## Lab12-5 Dynamic RNN

  - 들어오는 sequnce가 가변적임
  - 기존에는 가변적인 sequnce를 처리하기 위해 공백에는 <pad>를 추가
    - loss를 계산할때 문제가 생겨 성능이 낮아짐
  - Tensorflow에서는 Sequnce length를 미리 받아 sequnce length에 맞게 값을 출력하고 나머지 0처리
  ![Lec12_5-1](/assets/image/Lec12_5-1.JPG)
  ![Lec12_5-2](/assets/image/Lec12_5-2.JPG)

## Lab12-6 RNN with Time series data

  - Many to one 으로 설계
